---
title: Immutability
description: Why immutable data leads to safer, more predictable code
---

# Immutability

Immutability means data cannot be changed after creation. Instead of modifying existing data, you create new data with the desired changes. This simple principle transforms how you write and think about code, making programs more predictable and easier to understand.

When you work with mutable data, you're constantly tracking who might modify what and when. A list you pass to a function might come back changed. An object you store in one place might be modified somewhere else. This hidden coupling makes code hard to reason about and bugs hard to track down. Immutability eliminates these problems entirely.

## What Immutability Means

In traditional Python programming, you create data and then modify it. You append to lists, update dictionaries, and set attributes on objects. This seems natural, but every modification is a potential source of bugs. When you pass a list to a function, will the function modify it? When you store an object in multiple places, will changes in one place affect the other?

With immutability, none of these questions apply. Data never changes after creation. If you need to modify something, you create a new version with the changes. The original data remains intact and can be used elsewhere with confidence.

```python
# Mutable approach - modifies original
numbers = [1, 2, 3]
numbers.append(4)  # numbers is now [1, 2, 3, 4]

# Immutable approach - returns new data
from better_py import PersistentList

numbers = PersistentList.of(1, 2, 3)
new_numbers = numbers.append(4)
# numbers is still [1, 2, 3]
# new_numbers is [1, 2, 3, 4]
```

This might seem inefficient, but persistent data structures use structural sharing to make it practical. When you create a new version of an immutable data structure, the new version shares most of its structure with the original. Only the changed parts are newly allocated. This gives you the benefits of immutability without the performance penalty.

## Why Immutability Matters

The benefits of immutability show up every day in real code. The most immediate benefit is predictability. When data can't change, you never have to wonder what state it's in. A value is exactly what it was when you created it, nothing more and nothing less.

This predictability makes debugging dramatically easier. When you encounter a bug, you can trace through the code knowing that values don't change unexpectedly. You don't have to track down who modified a list or when a value changed. Each value has a clear history, and you can inspect any step in that history.

Immutability also eliminates entire classes of bugs. No more accidental modifications. No more surprising side effects. No more race conditions from concurrent access. When data can't change, all these problems simply disappear.

```python
# Mutable - surprising behavior
def process(items):
    items.clear()  # Oops! Modifies original

my_list = [1, 2, 3]
process(my_list)
print(my_list)  # [] - Empty! Unexpected

# Immutable - predictable behavior
def process(items):
    return items.map(lambda x: x * 2)

my_list = PersistentList.of(1, 2, 3)
new_list = process(my_list)
print(my_list)  # [1, 2, 3] - Unchanged
```

## Thread Safety Without Locks

Concurrency becomes much simpler with immutable data. Race conditions happen when two threads try to modify the same data simultaneously. With mutable data, you need locks to prevent this, and locks bring their own complexity. Deadlocks, priority inversions, and subtle timing bugs become constant concerns.

Immutable data eliminates all of this. When data can't be modified, there's nothing to race over. Multiple threads can read the same data simultaneously without any locks. If a thread needs to modify data, it creates a new version. Other threads continue using the original version unaffected.

```python
# Safe concurrent processing
from better_py import PersistentList
from threading import Thread

shared_data = PersistentList.of(1, 2, 3, 4, 5)

# Multiple threads can read safely
def process(data):
    # No locks needed - data won't change
    return data.map(lambda x: x * 2)

thread1 = Thread(target=process, args=(shared_data,))
thread2 = Thread(target=process, args=(shared_data,))
```

This doesn't mean all concurrency problems disappear. You still need to coordinate access to external resources like databases or files. but the data structure itself is safe. This eliminates a huge source of complexity in concurrent programs.

## Easier State Management

Managing state is one of the hardest problems in programming. State changes over time, and keeping track of all the changes is difficult. Who changed this value? When did it change? Why is it in this state now? These questions haunt developers working with mutable state.

Immutability transforms state management. Instead of modifying state in place, you represent each state as a value. Transitions create new state values. The old state values remain available for inspection. This gives you a complete history of how state evolved over time.

```python
# Mutable - hidden state changes
class Counter:
    def __init__(self):
        self.count = 0

    def increment(self):
        self.count += 1  # Hidden mutation

# Immutable - explicit changes
from dataclasses import dataclass

@dataclass(frozen=True)
class Counter:
    count: int

    def increment(self) -> "Counter":
        return Counter(self.count + 1)  # Returns new instance
```

This approach is particularly powerful in user interfaces. Each state of the UI is an immutable value. User interactions create new state values. You can easily implement undo by keeping a history of previous states. You can replay interactions to reproduce bugs. You can serialize the entire state for testing or debugging.

## Persistent Data Structures

Better-py provides persistent data structures that are both immutable and efficient. These data structures use structural sharing to avoid unnecessary copying. When you modify a persistent list, map, or set, the new version shares most of its structure with the original.

Structural sharing works by representing data as trees. Each modification creates new nodes along the path from the root to the modified element, but all other nodes are shared between the old and new versions. This means modifications are efficient, typically O(log n) instead of O(n).

```python
from better_py import PersistentList

# O(1) append with structural sharing
lst1 = PersistentList.of(1, 2, 3)
lst2 = lst1.append(4)

# lst1 and lst2 share most of their structure
# Only the new node is allocated
```

The efficiency of persistent data structures means you can use immutability everywhere, not just in performance-critical code. You don't have to choose between safety and performance. You get both.

## Working with PersistentList

PersistentList is an immutable list that supports all the operations you expect from a regular list, but without any modifications. Operations that would modify a regular list return a new PersistentList instead.

Creating a PersistentList is straightforward. You can create one from individual elements, from an existing iterable, or start with an empty list and build it up.

```python
from better_py import PersistentList

# Create from elements
lst = PersistentList.of(1, 2, 3)

# Create from iterable
lst = PersistentList.from_iter([1, 2, 3])

# Start empty
lst = PersistentList.empty()
```

Adding elements returns a new list without modifying the original. Append adds to the end, and prepend adds to the beginning. Both operations are efficient thanks to structural sharing.

```python
# Add - returns new list
new_lst = lst.append(4)

# Prepend - O(1) with structural sharing
new_lst = lst.prepend(0)
```

Accessing elements is safe and returns Maybe to handle out-of-bounds indices gracefully. Slicing returns a new list without affecting the original.

```python
# Access by index
value = lst.get(0)  # Returns Maybe.some(1)
value = lst.get(10)  # Returns Nothing

# Slice - returns new list
sub = lst.slice(1, 3)  # Returns PersistentList(2, 3)
```

Transformations return new lists, leaving the original unchanged. Map applies a function to each element. Filter keeps only elements matching a predicate. Both operations are lazy when possible and efficient.

```python
# Transform - returns new list
mapped = lst.map(lambda x: x * 2)  # [2, 4, 6]
filtered = lst.filter(lambda x: x > 1)  # [2, 3]
```

When you need to convert back to regular Python types, PersistentList provides conversion methods. This lets you use immutable data structures internally and convert only when interfacing with code that expects mutable types.

```python
# Convert to Python list
python_list = lst.to_list()  # [1, 2, 3]
```

## Working with PersistentMap

PersistentMap is an immutable hash map that provides the same safety and efficiency as PersistentList, but for key-value pairs. Like PersistentList, all operations return new maps without modifying the original.

Creating a PersistentMap works similarly to PersistentList. You can create one from a dictionary, from key-value pairs, or start empty and build it up.

```python
from better_py import PersistentMap

# Create from dictionary
m = PersistentMap({"a": 1, "b": 2})

# Create from pairs
m = PersistentMap.of_pairs([("a", 1), ("b", 2)])

# Start empty
m = PersistentMap.empty()
```

Getting values returns Maybe, making it safe to access keys that might not exist. Setting values returns a new map with the key set or updated. Deleting returns a new map without the key.

```python
# Get values - returns Maybe
value = m.get("a")  # Returns Maybe.some(1)
missing = m.get("z")  # Returns Nothing

# Set - returns new map
new_m = m.set("c", 3)  # Original unchanged

# Delete - returns new map
new_m = m.delete("b")  # Returns new map without "b"
```

Transformations work on values while preserving the immutable structure. Map_values applies a function to each value. Update sets multiple keys at once. Merge combines two maps.

```python
# Update multiple keys
updated = m.update({"c": 3, "d": 4})

# Transform values
mapped = m.map_values(lambda v: v * 2)  # {"a": 2, "b": 4}

# Merge with another map
other = PersistentMap({"b": 20, "c": 3})
merged = m.merge(other)  # {"a": 1, "b": 20, "c": 3}
```

## Working with PersistentSet

PersistentSet completes the trio of persistent collections, providing an immutable set with all the expected operations. Like PersistentList and PersistentMap, all operations return new sets without modifying the original.

Creating a PersistentSet follows the same pattern as the other persistent collections.

```python
from better_py import PersistentSet

# Create from elements
s = PersistentSet.of(1, 2, 3)

# Start empty
s = PersistentSet.empty()
```

Set operations add, remove, and check membership, all returning new sets when appropriate.

```python
# Add - returns new set
new_s = s.add(4)

# Remove - returns new set
new_s = s.remove(2)

# Check membership
has_2 = s.contains(2)  # Returns True
```

Set operations like union, intersection, and difference work as expected, returning new sets.

```python
other = PersistentSet.of(3, 4, 5)

union = s.union(other)  # {1, 2, 3, 4, 5}
intersection = s.intersection(other)  # {3}
difference = s.difference(other)  # {1, 2}
```

## Common Patterns

Persistent collections enable patterns that are cumbersome or unsafe with mutable data. One pattern is building state incrementally. Start with an empty collection and apply updates, keeping each intermediate state.

```python
from better_py import PersistentMap

# Start with empty
state = PersistentMap.empty()

# Build up state
state = state.set("user", user_data)
state = state.set("config", config_data)
state = state.set("session", session_data)

# Original empty state still available
```

Another pattern is history tracking. Keep a list of previous states to enable undo functionality or to audit changes over time.

```python
# Keep history of changes
history = []
current = PersistentMap.empty()

history.append(current)
current = current.set("x", 1)

history.append(current)
current = current.set("y", 2)

# Undo
current = history.pop()  # Go back to previous state
```

A third pattern is safe updates in APIs. When functions accept collections and return modified versions, callers don't have to worry about their original data being modified.

```python
def update_user(config: PersistentMap, key: str, value: any) -> PersistentMap:
    """Always returns new config, never mutates"""
    return config.set(key, value)

# Safe - original unchanged
old_config = load_config()
new_config = update_user(old_config, "theme", "dark")
```

<Callout type="success" title="When to Use Persistent Collections">

Persistent collections excel at application state, configuration data, immutable pipelines, concurrent access, and history tracking. These are situations where safety and predictability matter more than micro-optimizations.

</Callout>

<Callout type="warning" title="When to Use Mutable Collections">

For simple temporary lists, performance-critical inner loops, or interfacing with mutable APIs, regular Python collections might be more appropriate. Use good judgment and choose the right tool for the job.

</Callout>

## Performance Characteristics

Persistent collections have different performance characteristics than mutable collections, but they're competitive for most use cases. Access and modification operations are typically O(log n) instead of O(1), which is fast enough for most applications.

The real performance win comes from structural sharing. When you "modify" a persistent collection, you're not copying the entire structure. You're creating new nodes only for the changed path. Everything else is shared. This means modifications use minimal memory and are fast in practice.

| Operation | PersistentList | PersistentMap | PersistentSet |
|-----------|---------------|---------------|---------------|
| Access/Get | O(log n) | O(log n) | O(log n) |
| Add/Set | O(1) prepend | O(log n) | O(log n) |
| Delete | O(log n) | O(log n) | O(log n) |
| Length | O(1) | O(1) | O(1) |

## Common Mistakes

Working with immutable data requires thinking differently about how you structure code. Certain mistakes come up frequently when developers are transitioning from mutable to immutable patterns.

### Mistake 1: Mutating Persistent Collections

A common mistake is accidentally mutating persistent collections by converting them to mutable types and then modifying them. This breaks the guarantees that make persistent collections useful.

```python
# DON'T - Converts to mutable and mutates
def add_logging(items: PersistentList) -> PersistentList:
    mutable = items.to_list()
    mutable.append("[LOG]")  # Oops! Modified the mutable list
    return PersistentList.from_iter(mutable)

# DO - Uses persistent operations
def add_logging(items: PersistentList) -> PersistentList:
    return items.append("[LOG]")
```

The golden rule: never convert to mutable types unless absolutely necessary. When you must, do it at the boundaries and immediately convert back to persistent types.

### Mistake 2: Forgetting Return Values

With persistent collections, every operation returns a new collection. A common mistake is calling operations and ignoring the return value, then wondering why the original collection didn't change.

```python
# DON'T - Ignores return value
numbers = PersistentList.of(1, 2, 3)
numbers.append(4)  # Returns a new list, but we ignore it!
print(numbers)  # Still [1, 2, 3]

# DO - Captures return value
numbers = PersistentList.of(1, 2, 3)
numbers = numbers.append(4)
print(numbers)  # [1, 2, 3, 4]
```

This is a fundamental shift from mutable collections. You need to reassign the variable to capture the updated version.

### Mistake 3: Expecting In-Place Modifications

When working with libraries that expect mutable collections, it's easy to forget that persistent collections can't be modified in-place.

```python
# DON'T - Library might try to mutate
def sort_with_library(items: PersistentList) -> PersistentList:
    mutable_items = items.to_list()
    mutable_items.sort()  # Modifies in-place
    return PersistentList.from_iter(mutable_items)

# DO - Use operations that return new values
def sort_with_library(items: PersistentList) -> PersistentList:
    return items.sorted()  # Returns new sorted list
```

Better-py's persistent collections provide methods that return new collections instead of mutating in-place. Use those methods whenever possible.

### Mistake 4: Excessive Conversions

Converting back and forth between mutable and persistent collections is expensive and defeats the purpose of using persistent collections.

```python
# DON'T - Converts on every operation
def process_items(items: PersistentList) -> PersistentList:
    mutable = items.to_list()
    mutable.append(1)
    mutable.append(2)
    mutable.append(3)
    return PersistentList.from_iter(mutable)

# DO - Chains persistent operations
def process_items(items: PersistentList) -> PersistentList:
    return items.append(1).append(2).append(3)
```

Minimize conversions. Stay in the persistent world as much as possible, only converting at boundaries where you absolutely must.

### Mistake 5: Not Leveraging Structural Sharing

A common misconception is that every modification creates a full copy of the data structure. In reality, persistent collections use structural sharing to make modifications efficient.

```python
# This is efficient!
list1 = PersistentList.of(1, 2, 3)
list2 = list1.append(4)
list3 = list2.append(5)

# list1, list2, and list3 share most of their structure
# Only the new nodes are allocated
```

You don't need to worry about the performance implications of creating new versions. Structural sharing makes it efficient.

## When NOT to Use Immutable Data

Immutability is a powerful default, but it's not the right choice for every situation. Understanding when to use mutable data helps you write code that's both safe and efficient.

### Large Buffers and Binary Data

For large data buffers like images, video, or audio files, immutable persistent collections add overhead without providing much benefit. The data is usually treated as a blob rather than structured data, and mutations are rare or carefully controlled.

```python
# For image data, use bytes or bytearray, not PersistentList
def process_image(image_data: bytes) -> bytes:
    # Process the image
    return modified_data
```

Mutable types like `bytearray` make sense here because you're working with raw binary data, not structured collections.

### Performance-Critical Inner Loops

In tight loops where performance is absolutely critical and you've profiled to confirm that immutability is a bottleneck, mutable data structures might be appropriate.

```python
# In a performance-critical loop, direct list operations might be faster
def sum_squares(numbers: list[int]) -> int:
    total = 0
    for n in numbers:
        total += n * n
    return total
```

Profile before optimizing. The readability benefits of immutability usually outweigh small performance differences. Only use mutable data when profiling shows it's actually necessary.

### Interfacing with Mutable APIs

Some libraries and APIs expect mutable collections and don't provide any way to work with immutable data. Constantly converting back and forth can be cumbersome.

```python
# Work with the library's conventions
def process_with_pandas(data: PersistentList):
    mutable_list = data.to_list()
    df = pd.DataFrame(mutable_list)
    result = df.process().to_dict('records')
    return PersistentList.from_iter(result)
```

Convert at the boundary, do the mutable work, then convert back to persistent types immediately. Keep the mutable scope as small as possible.

### Simple Temporary Computations

For temporary, throwaway computations where the data doesn't leave the function and no shared state is involved, mutable collections can be simpler.

```python
# Simple computation - mutable is fine
def compute_statistics(values: list[float]) -> dict:
    sorted_values = sorted(values)
    return {
        "mean": sum(sorted_values) / len(sorted_values),
        "median": sorted_values[len(sorted_values) // 2],
    }
```

The data is local to the function, doesn't escape, and the computation is straightforward. Immutability doesn't provide much benefit here.

### Legacy Code Refactoring

When working with a large legacy codebase that uses mutable collections everywhere, trying to convert everything to immutable at once is overwhelming.

```python
# Gradual migration is fine
def new_function(data: PersistentList) -> PersistentList:
    # New code uses persistent collections
    return data.map(lambda x: x * 2)

def legacy_function(data: list) -> list:
    # Old code stays mutable for now
    result = []
    for item in data:
        result.append(process(item))
    return result
```

Use persistent collections in new code and refactor old code incrementally. You don't have to change everything at once.

<Callout type="info" title="Pragmatic Immutability">

Immutability is a principle, not a dogma. Use immutable data by default, especially for application state, shared data, and long-lived collections. Use mutable data where it makes sense: performance-critical code, large buffers, or when interfacing with mutable APIs. The goal is safer, more predictable code, not purity for its own sake.

</Callout>

## Making the Transition

Transitioning to immutable data doesn't have to be all or nothing. Start by using persistent collections in new code, especially for application state and configuration. Gradually refactor mutable code to use immutable structures where it makes sense.

You'll find that code becomes more predictable as you adopt immutability. Bugs become rarer. Refactoring becomes less scary. Concurrent code becomes simpler. These benefits compound over time, making immutability one of the most impactful changes you can make to how you write code.

Ready to dive deeper? The detailed documentation for each persistent collection shows you all the operations and patterns you need to use them effectively in real applications.
