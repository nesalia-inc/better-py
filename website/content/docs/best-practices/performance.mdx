---
title: Performance Best Practices
description: Write efficient functional code
---

# Performance Best Practices

Functional programming in Python doesn't mean slow code. Better-py is designed to be efficient, and following the right patterns ensures your code is both safe and fast. The key is understanding how persistent data structures work and using them appropriately.

Performance optimization requires measurement, not guessing. The patterns described here are based on how better-py's data structures actually work. But remember: profile first, optimize second. Most code is fast enough, and premature optimization is a waste of time.

## Understanding Persistent Data Structure Performance

Better-py's persistent collections have different performance characteristics than Python's built-in mutable collections. These differences stem from how they're implemented—using trees with structural sharing instead of contiguous arrays.

When you modify a persistent collection, you're not copying the entire structure. You're creating new nodes along the path from the root to the modified element. Everything else is shared between the old and new versions. This makes most operations O(log n) instead of O(1), but the constant factors are small enough that this rarely matters in practice.

The real performance win comes from sharing. When you "modify" a persistent collection, you're allocating only a few new nodes, not copying the entire structure. This means modifications use minimal memory and are fast in practice.

## Choosing the Right Operations

Not all operations are equally efficient. Understanding which operations are fast and which are slow helps you write performant code without sacrificing clarity.

### Prefer Prepend Over Append

PersistentList is optimized for prepending (adding to the front) because it's implemented as a linked list. Prepend is O(1) while append is O(log n).

```python
# Good: O(1) prepend
lst = PersistentList.empty()
for item in items:
    lst = lst.prepend(item)

# Avoid: O(log n) append in loop
lst = PersistentList.empty()
for item in items:
    lst = lst.append(item)  # Slower!
```

If you need to build a list by appending to the end, consider building it in reverse order using prepend, then reversing at the end:

```python
# Efficient append pattern
lst = PersistentList.empty()
for item in items:
    lst = lst.prepend(item)  # Fast prepend
lst = lst.reverse()  # Single O(log n) operation
```

### Build Collections in Bulk

When you have all the data upfront, build the collection in one operation instead of incrementally. Better-py can optimize bulk construction.

```python
# Good: Build once
lst = PersistentList.from_iter(large_list)

# Avoid: Building incrementally
lst = PersistentList.empty()
for item in large_list:
    lst = lst.append(item)
```

Bulk construction is faster because better-py can build an optimal tree structure directly, rather than rebalancing after each insertion.

### Use Appropriate Data Structures

Choose the right persistent collection for your use case. PersistentList excels at different operations than PersistentMap or PersistentSet.

```python
# Good: Use PersistentMap for lookups
user_map = PersistentMap.of_pairs([
    ("user1", user1_data),
    ("user2", user2_data),
])
user = user_map.get("user1")  # O(log n) lookup

# Avoid: Using PersistentList for lookups
users = PersistentList.of(user1_data, user2_data)
# O(n) search through list
```

PersistentMap and PersistentSet use hash-based tries with O(log n) lookups. PersistentList requires O(n) traversal to find an element. Use the right structure for your access pattern.

## Working with Python Collections

Sometimes you need to interoperate with mutable Python libraries. The key is to minimize conversions and do them at boundaries.

### Convert at Boundaries

When working with mutable libraries, convert right before calling them and convert back immediately after. Keep the mutable scope as small as possible.

```python
# Confine mutation to a small scope
def process_with_pandas(data: PersistentList) -> PersistentList:
    # Convert right before the call
    mutable_list = data.to_list()

    # Use the library
    df = pd.DataFrame(mutable_list)
    result = df.process().to_dict('records')

    # Convert back immediately
    return PersistentList.from_iter(result)
```

This confines the mutable code to a small, well-defined scope. The rest of your code remains fully immutable and type-safe.

### Use Native Python for Heavy Computation

For computation-heavy operations that Python's built-in functions optimize well, consider using regular Python types and converting the result.

```python
# Use native Python for bulk operations, then convert
def transform_all(items: PersistentList) -> PersistentList:
    # Convert once
    mutable = items.to_list()

    # Fast native computation
    result = [transform(x) for x in mutable]

    # Convert back
    return PersistentList.from_iter(result)
```

Python's list comprehensions and built-in functions are highly optimized. For bulk transformations, they can be faster than chaining persistent operations, especially for simple operations like mapping or filtering.

## Memory Efficiency

Persistent collections are memory-efficient thanks to structural sharing, but there are patterns that maximize this benefit.

### Share Structure Through Versions

When you keep multiple versions of a collection, structural sharing means you don't duplicate the shared parts.

```python
# Efficient: All versions share structure
state1 = PersistentMap({"count": 0, "name": "app"})
state2 = state1.set("count", 1)
state3 = state2.set("count", 2)

# state1, state2, and state3 share most of their structure
# Only the changed paths are newly allocated
```

This is perfect for maintaining history, implementing undo/redo, or managing state transitions. You can keep hundreds or thousands of versions without significant memory overhead.

### Avoid Excessive Intermediate Collections

Chaining operations creates intermediate collections. For very long chains, consider whether you can combine operations.

```python
# Creates intermediate collections
result = (
    lst.map(lambda x: x * 2)
        .map(lambda x: x + 1)
        .map(lambda x: x // 3)
)

# Better: Combine operations
result = lst.map(lambda x: (x * 2 + 1) // 3)
```

Each map call creates a new collection. Combining operations reduces the number of intermediate collections and improves performance.

## Lazy Evaluation

Better-py doesn't provide lazy evaluation primitives, but you can implement lazy patterns when working with large datasets.

### Use Generators for Large Pipelines

When processing large datasets, Python generators provide lazy evaluation that can be more efficient than eager persistent collections.

```python
# Lazy processing with generators
def process_large_file(filename: str):
    with open(filename) as f:
        for line in f:
            yield parse_line(line)

# Consume lazily
for item in process_large_file("large.txt"):
    result = process(item)
```

Generators process one item at a time, never loading the entire dataset into memory. This can be more efficient than loading everything into a PersistentList upfront.

### Batch Processing

For very large datasets, process in batches rather than loading everything at once.

```python
def process_in_batches(items: PersistentList, batch_size: int = 1000):
    for i in range(0, items.length(), batch_size):
        batch = items.slice(i, i + batch_size)
        process_batch(batch)
```

This balances memory usage with the convenience of persistent collections.

## Profiling and Measurement

The only way to know if code is performance-critical is to measure. Don't optimize based on guesses—profile to identify actual bottlenecks.

### Use Python Profilers

Python's built-in profilers identify slow functions and hot paths.

```bash
# Profile your code
python -m cProfile -o profile.stats your_script.py

# View results
python -c "import pstats; pstats.Stats('profile.stats').sort_stats('cumulative').print_stats(20)"
```

This tells you exactly where your code spends time. Optimize the functions that actually consume time, not the ones you think might be slow.

### Benchmark Critical Paths

For performance-critical code, use benchmarks to verify that optimizations actually help.

```python
import timeit

def benchmark_persistent_list():
    code = """
lst = PersistentList.of(1, 2, 3)
result = lst.map(lambda x: x * 2).filter(lambda x: x > 3)
"""
    time = timeit.timeit(code, setup="from better_py import PersistentList", number=10000)
    print(f"PersistentList: {time:.4f} seconds")
```

Run benchmarks before and after optimizations to verify they actually improve performance.

<Callout type="warning" title="Profile Before Optimizing">

Most code is fast enough. Only optimize when profiling shows a genuine bottleneck. Readability and correctness are more important than micro-optimizations.

</Callout>

## Common Performance Pitfalls

Understanding common mistakes helps you avoid them without sacrificing code quality.

### Premature Conversion

Converting between persistent and mutable types too frequently kills performance.

```python
# Bad: Converts on every operation
def slow_process(items: PersistentList):
    for item in items:
        mutable = items.to_list()  # Unnecessary conversion!
        mutable.append(item)
        items = PersistentList.from_iter(mutable)
    return items

# Good: Stays in persistent world
def fast_process(items: PersistentList):
    return items
```

Minimize conversions. Stay in the persistent world as much as possible.

### Ignoring Algorithmic Complexity

Functional programming doesn't change the fundamentals of algorithmic complexity. An O(n²) algorithm is still slow, even with persistent collections.

```python
# Bad: O(n²) nested lookups
def find_duplicates(items: PersistentList):
    result = []
    for i, item1 in enumerate(items.to_list()):
        for j, item2 in enumerate(items.to_list()):
            if i != j and item1 == item2:
                result.append(item1)
    return PersistentList.from_iter(result)

# Good: O(n) with a set
def find_duplicates(items: PersistentList):
    seen = set()
    duplicates = []
    for item in items.to_list():
        if item in seen:
            duplicates.append(item)
        seen.add(item)
    return PersistentList.from_iter(duplicates)
```

Choose algorithms with appropriate complexity, regardless of whether you're using functional or imperative style.

## Putting It All Together

Performance in functional Python comes from understanding your tools and using them appropriately. Persistent collections are efficient for most use cases, especially when you leverage structural sharing and choose the right operations.

Remember that readability and correctness are more important than micro-optimizations. Write clear, functional code first. Profile it. Only optimize the bottlenecks that actually matter. You'll find that most of the time, better-py's persistent collections are fast enough, and your code is safer and more maintainable for it.

<Callout type="info" title="See More Examples">

- [Collections Performance](/docs/collections/performance) - Detailed performance characteristics
- [PersistentList Guide](/docs/collections/persistent-list) - List-specific performance notes
- [Structural Sharing](/docs/(basics)/concepts/immutability) - How persistent collections share structure

</Callout>
